Here you go â€” a markdown **checklist of all the paper/folder names** in that repo:

```markdown
- [ ] 3D-ViTac: Learning Fine-Grained Manipulation with Visuo-Tactile Sensing  
- [ ] ALPHA- and Bi-ACT Are All You Need: Importance of Position and Force Information Control for Imitation Learning of Unimanual and Bimanual Robotic Manipulation with Low-Cost System  
- [ ] Adaptive Compliance Policy: Learning Approximate Compliance for Diffusion Guided Control  
- [ ] Binding Touch to Everything: Learning Unified Multimodal Tactile Representations  
- [ ] DexForce: Extracting Force-informed Actions from Kinesthetic Demonstrations for Dexterous Manipulation  
- [ ] EquiContact: A Hierarchical SE(3) Vision-to-Force Equivariant Policy for Spatially Generalizable Contact-rich Tasks  
- [ ] FACTR: Force-Attending Curriculum Training for Contact-Rich Policy Learning  
- [ ] Factory: Fast Contact for Robotic Assembly  
- [ ] FoAR: Force-Aware Reactive Policy for Contact-Rich Robotic Manipulation  
- [ ] ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation  
- [ ] FreeTacMan: Robot-free Visuo-Tactile Data Collection System for Contact-rich Manipulation  
- [ ] IndustReal: Transferring Contact-Rich Assembly Tasks from Simulation to Reality  
- [ ] Learning Hierarchical Domain Models Through Environment-Grounded Interaction  
- [ ] Learning Visuotactile Skills with Two Multifingered Hands  
- [ ] Learning to Grasp the Ungraspable with Emergent Extrinsic Dexterity  
- [ ] MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation  
- [ ] Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks  
- [ ] ManipForce: Force-Guided Policy Learning with Frequency-Aware Representation for Contact-Rich Manipulation  
- [ ] OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing  
- [ ] Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation  
- [ ] SERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning  
- [ ] TA-VLA: Elucidating the Design Space of Torque-aware Vision-Language-Action Models  
- [ ] TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction  
- [ ] Tactile-RL for Insertion: Generalization to Objects of Unknown Geometry  
- [ ] Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization  
- [ ] Touch begins where vision ends: Generalizable policies for contact-rich manipulation  
- [ ] Touch in the Wild: Learning Fine-Grained Manipulation with a Portable Visuo-Tactile Gripper  
- [ ] Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots  
- [ ] Variable Impedance Control in End-Effector Space: An Action Space for Reinforcement Learning in Contact-Rich Tasks  
- [ ] ViTaMIn: Learning Contact-Rich Tasks Through Robot-Free Visuo-Tactile Manipulation Interface  
- [ ] Watch Less, Feel More: Sim-to-Real RL for Generalizable Articulated Object Manipulation via Motion Adaptation and Impedance Control  
```

You can paste this into Notion, Obsidian, GitHub, etc., and tick things off as you go. If you want, next I can:

* add a second checkbox per item like `[ ] read  /  [ ] integrated into RCC`, or
* group them into categories (visuo-tactile, residual RL, sim2real, VLA, etc.).
